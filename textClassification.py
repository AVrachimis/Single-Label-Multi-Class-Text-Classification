# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

# Andreas Vrachimis

## Part A: Post Sentiment Classification ##
"""

import pandas as pd
import numpy as np

train_data = pd.read_json("https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_train.json")

validation_data = pd.read_json("https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_validation.json")

test_data = pd.read_json("https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_sentiment_test.json")

### The one_hot_vectorizer from the labs
import spacy

nlp = spacy.load('en_core_web_sm', disable=['ner'])
nlp.remove_pipe('tagger')
nlp.remove_pipe('parser')

#@Tokenize
def spacy_tokenize(string):
  tokens = list()
  doc = nlp(string)
  for token in doc:
    tokens.append(token)
  return tokens

#@Normalize
def normalize(tokens):
  normalized_tokens = list()
  for token in tokens:
    normalized = token.text.lower().strip()
    if ((token.is_alpha or token.is_digit)):
      normalized_tokens.append(normalized)
  return normalized_tokens
  return normalized_tokens

#@Tokenize and normalize
def tokenize_normalize(string):
  return normalize(spacy_tokenize(string))

from sklearn.feature_extraction.text import CountVectorizer

one_hot_vectorizer = CountVectorizer( tokenizer = tokenize_normalize, binary=True, max_features=20000 )

# Your code here
validation_data.head(20)

"""## Q1

Labels
"""

train_labels = train_data['sentiment.polarity']
validation_labels = validation_data['sentiment.polarity']
test_labels = test_data['sentiment.polarity']

"""Train Validation Test Split"""

len_train = train_labels.size
len_validation = validation_labels.size
len_test = test_labels.size
dataset_size = len_train+len_validation+len_test
print("Train Percentage: {:.2f}%".format(((len_train)/dataset_size)*100))
print("Validation Percentage: {:.2f}%".format((len_validation/(dataset_size))*100))
print("Test Percentage: {:.2f}%\n".format((len_test/dataset_size)*100))

"""Labels Distributions"""

labels = ['neutral','positive','negative','very positive','very negative']
# Train Set
# print("Train Set\n---------")
labels_counts = train_labels.value_counts()
total = train_labels.size
train = []
for i,count in enumerate(labels_counts):
  #print("{}:{:.2f}%".format(labels[i], (count/total)*100))
  train.append(count/total*100)

# Validation Set
# print("\nValidation Set\n---------")
labels_counts = validation_labels.value_counts()
total = validation_labels.size
validation=[]
for i,count in enumerate(labels_counts):
  #print("{}:{:.2f}%".format(labels[i], (count/total)*100))
  validation.append(count/total*100)


# Test Set
# print("\nTest Set\n---------")
labels_counts = test_labels.value_counts()
total = test_labels.size
test=[]
for i,count in enumerate(labels_counts):
  #print("{}:{:.2f}%".format(labels[i], (count/total)*100))
  test.append(count/total*100)


distribution = {'Train Set': train, 'Validation Set': validation, 'Test Set':test}
df = pd.DataFrame.from_dict(distribution)
df.index = labels
df.round(2)

"""### Vectorization <br>


1.   One Hot Encoding Vectorization
2.   TF - IDF Vectorization


"""

# 1. One Hot

train_features_one_hot = one_hot_vectorizer.fit_transform(train_data['body'])

validation_features_one_hot = one_hot_vectorizer.transform(validation_data['body'])
test_features_one_hot = one_hot_vectorizer.transform(test_data['body'])

# 2. TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf_vectorizer = TfidfVectorizer( tokenizer = tokenize_normalize, binary=True, max_features=20000 )

train_features_tf_idf = tf_idf_vectorizer.fit_transform(train_data['body'])

validation_features_tf_idf = tf_idf_vectorizer.transform(validation_data['body'])
test_features_tf_idf = tf_idf_vectorizer.transform(test_data['body'])

from sklearn.base import BaseEstimator, TransformerMixin

class ItemSelector(BaseEstimator, TransformerMixin):
    """For data grouped by feature, select subset of data at a provided key.    """

    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]

"""Function from lab 4 used to evaluate the models created"""

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import fbeta_score

def evaluation_summary(description, predictions, true_labels,print_report=True):
  if print_report:
    print("Evaluation for: " + description)
  precision = precision_score(predictions, true_labels, average='macro')
  recall = recall_score(predictions, true_labels,average='macro')
  accuracy = accuracy_score(predictions, true_labels)
  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure
  print("Classifier '%s' has \t Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f" % (description,accuracy,precision,recall,f1))
  if print_report:
    print(classification_report(predictions, true_labels, digits=3))
    print('\nConfusion matrix:\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted

"""### a) Dummy Classifier with strategy = "most frequent""""

from sklearn.dummy import DummyClassifier

mf_classifier = DummyClassifier(strategy="most_frequent")
mf_model = mf_classifier.fit(train_features_one_hot,train_labels)

evaluation_summary("Train Dataset:      Dummy Classifier Most Frequent", mf_model.predict(train_features_one_hot) , train_labels,print_report=False)
evaluation_summary("Validation Dataset: Dummy Classifier Most Frequent", mf_model.predict(validation_features_one_hot) , validation_labels)

"""### b) Dummy Classifier with strategy = "stratified"

"""

stra_classifier = DummyClassifier(strategy="stratified")
stra_model = stra_classifier.fit(train_features_one_hot,train_labels)

evaluation_summary("Train Dataset:      Dummy Classifier Stratified",  stra_model.predict(train_features_one_hot), train_labels, print_report=False)
evaluation_summary("Validation Dataset: Dummy Classifier Stratified",  stra_model.predict(validation_features_one_hot), validation_labels)

"""### c) Logistic Regression with One-hot vectorization"""

from sklearn.linear_model import LogisticRegression
lr_classifier_one_hot = LogisticRegression(max_iter=1000)
lr_model_one_hot = lr_classifier_one_hot.fit(train_features_one_hot,train_labels)

evaluation_summary("Train Dataset:      Logistic Regression with One Hot",  lr_model_one_hot.predict(train_features_one_hot), train_labels,print_report=False)
evaluation_summary("Validation Dataset: Logistic Regression with One Hot",  lr_model_one_hot.predict(validation_features_one_hot), validation_labels)

"""### d) Logistic Regression with TF-IDF vectorization"""

lr_classifier_tf_idf = LogisticRegression(max_iter=1000)

lr_model_tf_idf = lr_classifier_tf_idf.fit(train_features_tf_idf,train_labels)

evaluation_summary("Train Dataset:      Logistic Regression with TF-IDF",  lr_model_tf_idf.predict(train_features_tf_idf), train_labels,print_report = False)
evaluation_summary("Validation Dataset: Logistic Regression with TF-IDF",  lr_model_tf_idf.predict(validation_features_tf_idf), validation_labels)

"""### e) SVC Classifier with One-hot vectorization """

from sklearn.svm import SVC
svc_classifier = SVC(kernel='rbf')
svc_model_one_hot = svc_classifier.fit(train_features_one_hot,train_labels)

evaluation_summary("Train Dataset:      SVC Classifier with One Hot",  svc_model_one_hot.predict(train_features_one_hot), train_labels,print_report=False)
evaluation_summary("Validation Dataset: SVC Classifier with One Hot",  svc_model_one_hot.predict(validation_features_one_hot), validation_labels)

"""### f) Random Forest Classifier with TF-IDF Vectorization"""

from sklearn.ensemble import RandomForestClassifier

random_forest_classifier = RandomForestClassifier()
random_forest_model = random_forest_classifier.fit(train_features_tf_idf,train_labels)

evaluation_summary("Train Dataset:      Random Forest Classifier with TF-IDF",  random_forest_model.predict(train_features_tf_idf), train_labels,print_report = False)
evaluation_summary("Validation Dataset: Random Forest Classifier with TF-IDF",  random_forest_model.predict(validation_features_tf_idf), validation_labels)

"""i) Results on test sets"""

evaluation_summary("Dummy Classifier Most Frequent",    mf_model.predict(test_features_one_hot) ,                test_labels, print_report=False)
evaluation_summary("Dummy  Classifier  Stratified  ",   stra_model.predict(test_features_one_hot),               test_labels, print_report=False)
evaluation_summary("Logistic Regression with One Hot",  lr_model_one_hot.predict(test_features_one_hot),         test_labels, print_report=False)
evaluation_summary("Logistic Regression with TF-IDF",   lr_model_tf_idf.predict(test_features_tf_idf),           test_labels, print_report=False)
evaluation_summary("SVC Classifier  with  One  Hot",    svc_model_one_hot.predict(test_features_one_hot),        test_labels, print_report=False)
evaluation_summary("Random Forest CLassifier with TF-IDF", random_forest_model.predict(test_features_tf_idf),    test_labels, print_report=False)

"""### F1 Score per Label Graph
Logistic regression with One Hot has been the best performing classifier (by test macro F1) 
"""

from matplotlib import pyplot as plt

label = ['Negative','Neutral','Positive','Very Negative','Very Positive']

f1 = fbeta_score(lr_model_one_hot.predict(test_features_one_hot), test_labels ,1,average=None)


fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(label,f1)
plt.title("Logistic Regression with One Hot Vectorizer\n F1 Score per Label")
plt.ylabel('F1 score')
plt.show()

"""## Q2 <br>

### Parameter Tuning

Logistic Regression Parameters:


*   C = [0.001,0.01,1,10,1000,10000,100000]

#### **Parameter**: C
"""

# Logistic Regression Parameters
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')


parameter_C = {'lr_model__C': (0.001,0.01,1,10,1000,10000,100000)}


pipe = Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize)), 
              ('lr_model', LogisticRegression())
              ])


grid_search = GridSearchCV(pipe, param_grid=parameter_C, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(train_data, train_labels)

print("pipeline:", [name for name, _ in pipe.steps])
print("parameters:")
print(parameter_C)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_C.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""Vectorizer Parameters:

*   max_features = [None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000]
*   sublinear_tf = [True, False]
*   use_idf = [True, False]

#### **Parameter**: max_features
"""

parameter_max_features = {'tf-idf__max_features': (None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000)}

pipe = Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf=False)), 
              ('lr_model', LogisticRegression(C=10000))
              ])



grid_search = GridSearchCV(pipe, param_grid=parameter_max_features, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(train_data, train_labels)

print("pipeline:", [name for name, _ in pipe.steps])
print("parameters:")
print(parameter_max_features)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_max_features.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""#### **Parameter**: sublinear_tf"""

parameter_sublinear_tf = {'tf-idf__sublinear_tf': (True,False)}

pipe2 = Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000)), 
              ('lr_model', LogisticRegression(C=10000))
              ])


grid_search = GridSearchCV(pipe2, param_grid=parameter_sublinear_tf, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(train_data, train_labels)

print("pipeline:", [name for name, _ in pipe2.steps])
print("parameters:")
print(parameter_sublinear_tf)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_sublinear_tf.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""#### **Parameter**: use_idf"""

parameter_use_idf = {'tf-idf__use_idf': (True,False)}
pipe = Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf-idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False)), 
              ('lr_model', LogisticRegression(C=10000))
              ])



grid_search = GridSearchCV(pipe, param_grid=parameter_use_idf, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(train_data, train_labels)

print("pipeline:", [name for name, _ in pipe.steps])
print("parameters:")
print(parameter_use_idf)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_use_idf.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""Best Parameters:


*   Logistic Regression: C = 10000
*   TF-IDF: max_features = 2000
*   TF-IDF: sublinear_tf = False
*   TF_IDF: use_idf  = True

### Improved Model
"""

import warnings
warnings.filterwarnings('ignore')

tf_idf_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False,use_idf=True)

train_features_tuning_model = tf_idf_vectorizer.fit_transform(train_data['body'].values.astype('U'))

validation_features_tuning_model = tf_idf_vectorizer.transform(validation_data['body'].values.astype('U'))
test_features_tuning_model = tf_idf_vectorizer.transform(test_data['body'].values.astype('U'))

lr_model_tuning = LogisticRegression(C=10000)
lr_model_tuning_tf_idf = lr_model_tuning.fit(train_features_tuning_model,train_labels)

evaluation_summary("Improved Logistic Regression with TF-IDF",lr_model_tuning_tf_idf.predict(test_features_tuning_model),test_labels)

"""### Error Analysis

"""

validation_prediction = lr_model_tuning_tf_idf.predict(validation_features_tuning_model)

data = {'Label_Sentiment_Polarity': validation_labels,
        'Body':validation_data['body'],
        'Prediction': validation_prediction,
        }

dataf = pd.DataFrame (data, columns = ['Label_Sentiment_Polarity','Body','Prediction'])

dataf.head(50)

validation_prediction = lr_model_tuning_tf_idf.predict(validation_features_tuning_model)
evaluation_summary("Train Data:      Improved Logistic Regression with TF-IDF",lr_model_tuning_tf_idf.predict(train_features_tuning_model),train_labels,print_report=False)
evaluation_summary("Validation Data: Improved Logistic Regression with TF-IDF",lr_model_tuning_tf_idf.predict(validation_features_tuning_model),validation_labels,print_report=False)
evaluation_summary("Test Data:       Improved Logistic Regression with TF-IDF",lr_model_tuning_tf_idf.predict(test_features_tuning_model),test_labels,print_report=False)

"""## Q3

Add two features to(try to) improve sentiment polarity classification performance


* Feature 1: FeatureUnion body + majority_type
* Feature 2: Word Embeddings

### Feature 1: FeatureUnion
body + majority_type
"""

from sklearn.pipeline import FeatureUnion
from sklearn.pipeline import Pipeline

# Replace np.nan values of all the columns with empty string
train_data = train_data.replace(np.nan, ' ', regex=True)
validation_data = validation_data.replace(np.nan, ' ', regex=True)
test_data = test_data.replace(np.nan, ' ', regex=True)

# Use FeatureUnion to combine the features from body and majority_type
prediction_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
            ('majority_type', Pipeline([
              ('selector', ItemSelector(key='majority_type')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
        ])
        )
    ])

train_features_tf_idf = prediction_pipeline.fit_transform(train_data)
validation_features_tf_idf = prediction_pipeline.transform(validation_data)
test_features_tf_df = prediction_pipeline.transform(test_data)

lr_classifier = LogisticRegression(C=10000)
combined_lr_model = lr_classifier.fit(train_features_tf_idf,train_labels)
evaluation_summary("Combined LR TFIDF ", combined_lr_model.predict(test_features_tf_df), test_labels)

"""### Feature 2: Word Embedding"""

import gensim
from sklearn.base import BaseEstimator

class Estimator(BaseEstimator):

    def fit(self, X, y=None):
        """
        Accept input data, X, and optional target data, y. Returns self.
        """
        return self

    def predict(self, X):
        """
        Accept input data, X and return a vector of predictions for each row.
        """
        return yhat
      
# from sklearn.base import TransformerMixin

class Transfomer(BaseEstimator):

    def fit(self, X, y=None):
        """
        Learn how to transform data based on input data, X.
        """
        return self

    def transform(self, X):
        """
        Transform X into a new dataset, Xprime and return it.
        """
        return Xprime

import numpy as np

from sklearn.base import BaseEstimator #, TransformerMixin

class MaximumEmbeddingVectorizer(BaseEstimator): #, TransformerMixin
    def __init__(self, embedding_model):
        self.embedding = embedding_model
        self.dimension = embedding_model.vector_size

    def fit(self, X,y):
        return self
      
    def transform(self, X):  
      # Skip OOV terms. Return 0 if no words are in the vocabulary.
      #print (X)
      return np.array([ 
          np.max([self.embedding[token] for token in doc if token in self.embedding]
                or [np.zeros(self.dimension)], axis=0)
          for doc in X
      ])

import itertools

# train_data['body']

all_tokens_train = []
all_posts_tokenized_train = train_data.body.apply(tokenize_normalize)
all_tokens_train = list(itertools.chain.from_iterable(all_posts_tokenized_train))
print("Num tokens: ", len(all_tokens_train))


# validation_data['body']

all_tokens_validation = []
all_posts_tokenized_validation = validation_data.body.apply(tokenize_normalize)
all_tokens_validation = list(itertools.chain.from_iterable(all_posts_tokenized_validation))
print("Num tokens: ", len(all_tokens_validation))


# test_data['body']

all_tokens_test = []
all_posts_tokenized_test = test_data.body.apply(tokenize_normalize)
all_tokens_test = list(itertools.chain.from_iterable(all_posts_tokenized_test))
print("Num tokens: ", len(all_tokens_test))

from sklearn.metrics.pairwise import cosine_similarity
 
# A function that given an input query item returns the top-k most similar items 
# by their cosine similarity.
# From lab 5
def find_similar(query_vector, vd_matrix, top_k = 5):
    cosine_similarities = cosine_similarity(query_vector, vd_matrix).flatten()
    related_doc_indices = cosine_similarities.argsort()[::-1]
    return [(index, cosine_similarities[index]) for index in related_doc_indices][0:top_k]

import time

train_all_posts_tokenized = train_data.body.apply(tokenize_normalize)
test_all_posts_tokenized = test_data.body.apply(tokenize_normalize)

t0 = time.time()

train_model = gensim.models.Word2Vec(train_all_posts_tokenized, size=200, window=5, min_count=5, sg=0, alpha=0.025, iter=10, batch_words=10000)

t1 = time.time()
print ("Train Model Done in: %.02f s" % (t1 - t0))

def similiar_posts_feature(df, model, all_posts_tokenized,k=7):

  reddit_vectorizer = MaximumEmbeddingVectorizer(model)
  reddit_post_vector_matrix = reddit_vectorizer.transform(all_posts_tokenized)

  results = []
  for i,tokens in enumerate(all_posts_tokenized):

    transformed = reddit_vectorizer.transform([tokens])
    doc = transformed[0:1]

    similar_posts_string = ""
    count_similar=0
    for index, score in find_similar(doc, reddit_post_vector_matrix, k)[1:]:

      similar_post_polarity = df.iloc[index]['sentiment.polarity']

      # check the polarity of the similar post if equal with the polarity of the current post
      # Allow at maximum 2 post to be added on the 
      if similar_post_polarity == df.iloc[i]['sentiment.polarity']:

        count_similar+=1
        post_contents = df.iloc[index]['body'].replace('\n', ' ')
        similar_posts_string += post_contents

        # max 2 similar posts
        if count_similar == 2:
          break 

    results.append(similar_posts_string)
  return results

train_similar_body = similiar_posts_feature(train_data,train_model,train_all_posts_tokenized)

test_similar_body = similiar_posts_feature(test_data,train_model,test_all_posts_tokenized)

we_train_data = train_data.copy()
we_test_data = test_data.copy()

# add the new columns to the dataframes
we_train_data['Similar_Body'] = train_similar_body
we_test_data['Similar_Body'] = test_similar_body
we_train_data

pipe_word_embedding = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
        ])
        )
    ])

train_features_tf_idf_word_embedding = pipe_word_embedding.fit_transform(we_train_data)
test_features_tf_idf_word_embedding = pipe_word_embedding.transform(we_test_data)

word_embedding_train_labels = we_train_data['sentiment.polarity']
word_embedding_test_labels = we_test_data['sentiment.polarity']

lr_classifier = LogisticRegression(C=10000)
combined_lr_model_word_embedding = lr_classifier.fit(train_features_tf_idf_word_embedding,word_embedding_train_labels)
evaluation_summary("Combined LR TFIDF ", combined_lr_model_word_embedding.predict(test_features_tf_idf_word_embedding), word_embedding_test_labels)

"""### Combination of the Two Features 
FeatureUnion + Word Embedding
"""

# Replace np.nan with empty string
we_train_data = we_train_data.replace(np.nan, ' ', regex=True)
validation_data = validation_data.replace(np.nan, ' ', regex=True)
we_test_data = we_test_data.replace(np.nan, ' ', regex=True)
# Labels
f12_train_labels = we_train_data['sentiment.polarity']
f12_test_labels = we_test_data['sentiment.polarity']

f12_prediction_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
            ('majority_type', Pipeline([
              ('selector', ItemSelector(key='majority_type')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=2000,sublinear_tf = False, use_idf=True)), 
              ])),
        ])
        )
    ])

f12_train_features_tf_idf = f12_prediction_pipeline.fit_transform(we_train_data)
f12_test_features_tf_idf = f12_prediction_pipeline.transform(we_test_data)

lr_classifier = LogisticRegression(C=10000)
f12_combined_lr_model = lr_classifier.fit(f12_train_features_tf_idf,f12_train_labels)
evaluation_summary("Combined Features LR TFIDF ", f12_combined_lr_model.predict(f12_test_features_tf_idf), f12_test_labels)

from sklearn.metrics import plot_confusion_matrix

matrix = plot_confusion_matrix(f12_combined_lr_model,f12_test_features_tf_idf,f12_test_labels, display_labels=f12_test_labels.unique(),cmap=plt.cm.Blues)

plt.title("Confusion matrix of Logistic Regression with the two features combined")
plt.xticks(rotation=25)

"""# Part B: Thread Subreddit Prediction ##"""

import pandas as pd

subreddit_train_data = pd.read_json("https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_discourse_train.json")
subreddit_test_data = pd.read_json("https://raw.githubusercontent.com/rpsoft/tad_course/main/reddit_discourse_test.json")

# Top 20 subreddits are filtered here for you.
top_subreddits = subreddit_train_data.subreddit.value_counts().head(20)

subreddit_train_data = subreddit_train_data[subreddit_train_data['subreddit'].isin(top_subreddits.keys())]
subreddit_test_data = subreddit_test_data[subreddit_test_data['subreddit'].isin(top_subreddits.keys())]
subreddit_train_data['post_depth'] = subreddit_train_data['post_depth'].astype(str)
subreddit_test_data['post_depth'] = subreddit_test_data['post_depth'].astype(str)

subreddit_train_data

subreddit_train_data["subreddit"].value_counts()

"""#### Training / Validation data

For part B the training is to be performed in a utilising cross-validation. See Lab 4 for an example, in particular the use of GridSearchCV.

## Q4

### Pre-Processing
Handle missing values
"""

def remove_body_nan_rows(df):
  """function to remove all the rows that their body column is nan"""
  columns = ['body']
  df = df.replace(r'^\s*$', np.nan, regex=True)
  for column in columns:
    df = df[pd.notnull(df[column])]
  return df

print("Empty/Null Values Before:")
print("Null Body Values=",subreddit_train_data['body'].isnull().values.sum())
print("Empty Body Values=",subreddit_test_data['body'].eq("").values.sum())

subreddit_train_data = remove_body_nan_rows(subreddit_train_data).copy()
subreddit_test_data = remove_body_nan_rows(subreddit_test_data).copy()

print("\nEmpty/Null Values After:")
print("Null Body Values=",subreddit_train_data['body'].isnull().values.sum())
print("Empty Body Values=",subreddit_test_data['body'].eq("").values.sum())



# print("Null Body Values=",f1_subreddit_train_data['Similar_Body'].eq("").values.sum())

from sklearn.base import BaseEstimator, TransformerMixin

class ItemSelector(BaseEstimator, TransformerMixin):
    """For data grouped by feature, select subset of data at a provided key.    """

    def __init__(self, key):
        self.key = key

    def fit(self, x, y=None):
        return self

    def transform(self, data_dict):
        return data_dict[self.key]

"""Labels





"""

subreddit_train_labels = subreddit_train_data['subreddit']
subreddit_test_labels = subreddit_test_data['subreddit']

"""### Vectorizer

TF-IDF Vectorizer (on title,body,author) <br>
sublinear_tf=True
"""

from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer

subreddit_train_data = subreddit_train_data.replace(np.nan, ' ', regex=True)
subreddit_test_data = subreddit_test_data.replace(np.nan, ' ', regex=True)

subreddit_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

subreddit_train_features_tf_idf = subreddit_pipeline.fit_transform(subreddit_train_data)

subreddit_test_features_tf_df = subreddit_pipeline.transform(subreddit_test_data)

"""### Classifier
Logistic Regression
- L2 regularization
- C=10

"""

from sklearn.linear_model import LogisticRegression

subreddit_lr_classifier = LogisticRegression(C=10,penalty='l2')
subreddit_lr_model = subreddit_lr_classifier.fit(subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Subreddit LR TFIDF classifier ", subreddit_lr_model.predict(subreddit_test_features_tf_df), subreddit_test_labels)

"""### Three Classifiers chosen
1. SVC Classifier
2. Bernoulli Naive Bayes classifier
3. Decision Tree Classifier

#### i. SVC Classifier
"""

from sklearn.svm import SVC

subreddit_svc_classifier = SVC(kernel='rbf')
subreddit_svc_model_tf_idf = subreddit_svc_classifier.fit(subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Subreddit SVC Classifier with TF-IDF",  subreddit_svc_model_tf_idf.predict(subreddit_test_features_tf_df),subreddit_test_labels)

"""##### SVC Parameter Tuning

SVC Parameters:
- kernel = ['linear','poly','rbf','sigmoid','precomputed']
- C = [0.01,0.1,1,10,100]
"""

from sklearn.model_selection import GridSearchCV

import warnings
warnings.filterwarnings('ignore')


parameter_kernel = {'svc__kernel':('linear','poly','rbf','sigmoid','precomputed')}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('svc', SVC(C=10))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_kernel, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_kernel)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_kernel.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

import warnings
warnings.filterwarnings('ignore')

parameter_C = {'svc__C':  (0.01,0.1,1,10,100) }


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('svc', SVC(kernel='sigmoid'))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_C, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_C)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_C.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""Vectorizer Parameter
- max_features = [None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000]

"""

import warnings
warnings.filterwarnings('ignore')

parameter_max_features = {'union__body__tf_idf__max_features':  (None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000)}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('svc', SVC(kernel='sigmoid'))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_max_features, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_max_features)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_max_features.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""##### Improved SVC Model
* Classifier Parameter
  - kernel = 'sigmoid'
  - C = 10

* Vectorizer Parameter
  - max_features = 5000
"""

SVC_subreddit_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
        ])
        )
    ])
SCV_subreddit_train_features_tf_idf = SVC_subreddit_pipeline.fit_transform(subreddit_train_data)

SCV_subreddit_test_features_tf_df = SVC_subreddit_pipeline.transform(subreddit_test_data)

from sklearn.svm import SVC

subreddit_svc_classifier = SVC(kernel='sigmoid',C=10)
subreddit_svc_improved_model_tf_idf = subreddit_svc_classifier.fit(SCV_subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Subreddit SVC Classifier with TF-IDF",  subreddit_svc_improved_model_tf_idf.predict(SCV_subreddit_test_features_tf_df),subreddit_test_labels)

"""#### ii. Bernoulli Naive Bayes"""

from sklearn.naive_bayes import BernoulliNB
subreddit_nb_classifier = BernoulliNB()

subreddit_nb_model_tf_idf = subreddit_nb_classifier.fit(subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Bernoulli Naive Bayes with TF-IDF",  subreddit_nb_model_tf_idf.predict(subreddit_test_features_tf_df), subreddit_test_labels)

"""##### Bernoulli Naive Bayes Parameter Tuning

BernoulliNB Parameters:
- fit_prior = [True,False]
- alpha = [0.001,0.005,0.01,0.015,0.019,0.02,0.022,1]

**Parameter** fit_prior
"""

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV


parameter_fit_priorbool = {'NB__fit_prior': (True,False)}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('NB', BernoulliNB())
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_fit_priorbool, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_fit_priorbool)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_fit_priorbool.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""**Parameter** alpha"""

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV


parameter_fit_priorbool = {'NB__alpha': (0.001,0.005,0.01,0.015,0.019,0.02,0.022,1)}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('NB', BernoulliNB(fit_prior=False))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_fit_priorbool, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_fit_priorbool)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_fit_priorbool.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""Vectorizer Parameter
- max_features = [None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000]

"""

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV


parameter_max_features = {'union__body__tf_idf__max_features':  (None,1,10,100,1000,1300,2000,5000,10000,20000,30000,50000),
                    }


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('NB', BernoulliNB(alpha=0.02,fit_prior=False))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_max_features, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_max_features)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_max_features.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""##### Improved Bernoulli Naive Bayes and TF-IDF Vectorizer
* Classifier Parameter
  - alpha = 0.02
  - fit_prior = False
* Vectorizer Parameter
  - max_features = None
"""

from sklearn.naive_bayes import BernoulliNB

improved_subreddit_nb_classifier = BernoulliNB(alpha=0.02,fit_prior=False)

improved_bernoulli_subreddit_nb_model_tf_idf = improved_subreddit_nb_classifier.fit(subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Improved Bernoulli Naive Bayes with TF-IDF",  improved_bernoulli_subreddit_nb_model_tf_idf.predict(subreddit_test_features_tf_df), subreddit_test_labels)

"""#### iii. Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
subreddit_dtree_classifier = DecisionTreeClassifier(random_state=0)

subreddit_dtree_mode_tf_idf = subreddit_dtree_classifier.fit(subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Subreddit Decision Tree with TD-IDF",  subreddit_dtree_mode_tf_idf.predict(subreddit_test_features_tf_df), subreddit_test_labels)

"""##### Decision Tree Parameter Tuning

Decision tree Parameters:
- max_features = [None,'auto','sqrt','log2',5000,10000,15000,20000,25000,50000]
- random_state = [1,2,3,4]

**Parameter** max_features
"""

import warnings
warnings.filterwarnings('ignore')

parameter_max_features = {'decisionTree__max_features': (None,'auto','sqrt','log2',5000,10000,15000,20000,25000,50000)}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('decisionTree', DecisionTreeClassifier())
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_max_features, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_max_features)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_max_features.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""**Paremeter** random_state"""

import warnings
warnings.filterwarnings('ignore')

parameter_radnom_state = {'decisionTree__random_state': (1,2,3,4)}


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('decisionTree', DecisionTreeClassifier(max_features=20000))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_radnom_state, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_radnom_state)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_radnom_state.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""Vectorizer Parameter
- max_features = [None,1,10,100,1000,5000,10000,20000,50000,60000]

"""

import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import GridSearchCV


parameter_kernel = {'union__body__tf_idf__max_features':  (None,1,10,100,1000,5000,10000,20000,50000,60000),
                    }


pipeline_feature_union = Pipeline([
        ('union', FeatureUnion(
          [
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        ),
        ('decisionTree', DecisionTreeClassifier(random_state=1,max_features=20000))
    ])


grid_search = GridSearchCV(pipeline_feature_union, param_grid=parameter_kernel, n_jobs=1, verbose=1, scoring='f1_macro', cv=2)
grid_search.fit(subreddit_train_data, subreddit_train_labels)

print("pipeline:", [name for name, _ in pipeline_feature_union.steps])
print("parameters:")
print(parameter_kernel)

print("Best score: %0.3f" % grid_search.best_score_)
print("Best parameters set:")
best_parameters = grid_search.best_estimator_.get_params()
for param_name in sorted(parameter_kernel.keys()):
  print("\t%s: %r" % (param_name, best_parameters[param_name]))

"""##### Improved Decision Tree
* Classifier Parameter
  - max_features = 20000
  - random_state = 1
* Vectorizer Parameter
  - max_features = None
"""

improved_dt_subreddit_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize, sublinear_tf = True)), 
              ])),
        ])
        )
    ])

dt_subreddit_train_features_tf_idf = improved_dt_subreddit_pipeline.fit_transform(subreddit_train_data)

dt_subreddit_test_features_tf_df = improved_dt_subreddit_pipeline.transform(subreddit_test_data)

from sklearn.tree import DecisionTreeClassifier

subreddit_dtree_classifier = DecisionTreeClassifier(random_state=1,max_features=20000)

subreddit_dtree_mode_tf_idf = subreddit_dtree_classifier.fit(dt_subreddit_train_features_tf_idf,subreddit_train_labels)

evaluation_summary("Improved Subreddit Decision Tree with TD-IDF",  subreddit_dtree_mode_tf_idf.predict(dt_subreddit_test_features_tf_df), subreddit_test_labels)

"""### Chosen Classifiers Performance on test data"""

evaluation_summary("Subreddit Logistic Regression with TFIDF", subreddit_lr_model.predict(subreddit_test_features_tf_df), subreddit_test_labels,print_report=False) 
evaluation_summary("Improved SVC with TF-IDF\t\t\t",  subreddit_svc_improved_model_tf_idf.predict(SCV_subreddit_test_features_tf_df),subreddit_test_labels, print_report=False)    
evaluation_summary("Improved Bernoulli Naive Bayes with TF-IDF",  improved_bernoulli_subreddit_nb_model_tf_idf.predict(subreddit_test_features_tf_df), subreddit_test_labels,print_report=False)
evaluation_summary("Improved Decision Tree with TD-IDF\t\t",  subreddit_dtree_mode_tf_idf.predict(dt_subreddit_test_features_tf_df), subreddit_test_labels,print_report=False)

"""### Additional Features

#### Feature 1: Word Embedding
"""

import gensim
from sklearn.base import BaseEstimator

class Estimator(BaseEstimator):

    def fit(self, X, y=None):
        """
        Accept input data, X, and optional target data, y. Returns self.
        """
        return self

    def predict(self, X):
        """
        Accept input data, X and return a vector of predictions for each row.
        """
        return yhat
      
# from sklearn.base import TransformerMixin

class Transfomer(BaseEstimator):

    def fit(self, X, y=None):
        """
        Learn how to transform data based on input data, X.
        """
        return self

    def transform(self, X):
        """
        Transform X into a new dataset, Xprime and return it.
        """
        return Xprime

import numpy as np

from sklearn.base import BaseEstimator #, TransformerMixin

class MaximumEmbeddingVectorizer(BaseEstimator): #, TransformerMixin
    def __init__(self, embedding_model):
        self.embedding = embedding_model
        self.dimension = embedding_model.vector_size

    def fit(self, X,y):
        return self
      
    def transform(self, X):  
      # Skip OOV terms. Return 0 if no words are in the vocabulary.
      #print (X)
      return np.array([ 
          np.max([self.embedding[token] for token in doc if token in self.embedding]
                or [np.zeros(self.dimension)], axis=0)
          for doc in X
      ])

from sklearn.metrics.pairwise import cosine_similarity
 
# A function that given an input query item returns the top-k most similar items 
# by their cosine similarity.
# From lab 5
def find_similar(query_vector, vd_matrix, top_k = 5):
    cosine_similarities = cosine_similarity(query_vector, vd_matrix).flatten()
    related_doc_indices = cosine_similarities.argsort()[::-1]
    return [(index, cosine_similarities[index]) for index in related_doc_indices][0:top_k]

def subreddit_similiar_posts_feature(df, model, all_posts_tokenized,k=15):

  reddit_vectorizer = MaximumEmbeddingVectorizer(model)
  reddit_post_vector_matrix = reddit_vectorizer.transform(all_posts_tokenized)

  results = []
  for i,tokens in enumerate(all_posts_tokenized):

    transformed = reddit_vectorizer.transform([tokens])
    doc = transformed[0:1]

    similar_posts_string = ""
    count_similar=0
    similar = find_similar(doc, reddit_post_vector_matrix, k)[1:]
    top1 = df.iloc[similar[0][0]]['body']
    top2 = df.iloc[similar[1][0]]['body']

    for index, score in similar:

      similar_post_subreddit = df.iloc[index]['subreddit']

      """
      check if the subreddit category of the similar posts is equal with the subreddit category of the current post
      allow bodies with the same subreddit category as the current/[i]th body
      allow at maximum 2 post to be added
      in case of an empty similar string, the two most similar ones, descpite the category, are chosen
      """
      
      if similar_post_subreddit == df.iloc[i]['subreddit']:

        count_similar+=1
        post_contents = df.iloc[index]['body'].replace('\n', ' ')
        similar_posts_string += post_contents

        # max 2 similar posts
        if count_similar == 2:
          break 

    if similar_posts_string=="":
      similar_posts_string = "{} {}".format(top1,top2)

    results.append(similar_posts_string)
  return results

import time

train_all_posts_tokenized = subreddit_train_data.body.apply(tokenize_normalize)
test_all_posts_tokenized = subreddit_test_data.body.apply(tokenize_normalize)

t0 = time.time()

subreddit_train_model = gensim.models.Word2Vec(train_all_posts_tokenized, size=200, window=5, min_count=5, sg=0, alpha=0.025, iter=10, batch_words=10000)

t1 = time.time()
print ("Train Model Done in: %.02f s" % (t1 - t0))

t0 = time.time()

train_similar_body_1 = subreddit_similiar_posts_feature(subreddit_train_data,subreddit_train_model,train_all_posts_tokenized)
train_similar_body = train_similar_body_1
t1 = time.time()
print ("Train Model Done in: %.02f s" % (t1 - t0))

test_similar_body_1 = subreddit_similiar_posts_feature(subreddit_test_data,subreddit_train_model,test_all_posts_tokenized)
test_similar_body = test_similar_body_1
t2 = time.time()
print ("Train Model Done in: %.02f s" % (t2-t1))

f1_subreddit_train_data = subreddit_train_data.copy()
f1_subreddit_test_data = subreddit_test_data.copy()

# add the new columns to the dataframes
f1_subreddit_train_data['Similar_Body'] = train_similar_body
f1_subreddit_test_data['Similar_Body'] = test_similar_body
f1_subreddit_train_data

"""Vectorizer for Feature 1"""

f1_subreddit_pipe = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

f1_SVC_subreddit_pipe = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

"""Labels for Feature 1"""

# vectorizer for all classifiers except SVC
f1_subreddit_train_features = f1_subreddit_pipe.fit_transform(f1_subreddit_train_data)
f1_subreddit_test_features = f1_subreddit_pipe.transform(f1_subreddit_test_data)

# SVC vectorizer
f1_SVC_subreddit_train_features = f1_SVC_subreddit_pipe.fit_transform(f1_subreddit_train_data)
f1_SVC_subreddit_test_features = f1_SVC_subreddit_pipe.transform(f1_subreddit_test_data)

f1_subreddit_train_labels = f1_subreddit_train_data['subreddit']
f1_subreddit_test_labels = f1_subreddit_test_data['subreddit']

"""**Word Embeddings using SVC Classifier**"""

from sklearn.svm import SVC

f1_subreddit_svc_classifier = SVC(kernel='sigmoid',C=10)
f1_subreddit_svc_model_tf_idf = f1_subreddit_svc_classifier.fit(f1_SVC_subreddit_train_features, f1_subreddit_train_labels)

evaluation_summary("Feature 1: Subreddit SVC Classifier with TF-IDF",  f1_subreddit_svc_model_tf_idf.predict(f1_SVC_subreddit_test_features),f1_subreddit_test_labels)

"""**Word Embeddings using BernoulliNB Classifier**"""

from sklearn.naive_bayes import BernoulliNB

f1_subreddit_nb_classifier = BernoulliNB(alpha=0.02,fit_prior=False)

subreddit_nb_model_tf_idf = f1_subreddit_nb_classifier.fit(f1_subreddit_train_features,f1_subreddit_train_labels)

evaluation_summary("Feature 1: Bernoulli Naive Bayes with TF-IDF",  subreddit_nb_model_tf_idf.predict(f1_subreddit_test_features), f1_subreddit_test_labels)

"""**Word Embeddings using Decision Tree Classifier**"""

f1_subreddit_dtree_classifier = DecisionTreeClassifier(random_state=1,max_features=20000)

f1_subreddit_dtree_mode_tf_idf = f1_subreddit_dtree_classifier.fit(f1_subreddit_train_features,f1_subreddit_train_labels)

evaluation_summary("Feature 1: Subreddit Decision Tree with TD-IDF",  f1_subreddit_dtree_mode_tf_idf.predict(f1_subreddit_test_features), f1_subreddit_test_labels)

"""<a name="1"></a>

#### Feature 2: FeatureUnion - Post Depth 

Using FeatureUnion combine the columns `title`,`author`,`body` and `post_depth`
"""

subreddit_train_data

"""Vectorizer for Feature 2"""

f2_subreddit_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('post_depth', Pipeline([
              ('selector', ItemSelector(key='post_depth')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,lowercase = False,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

f2_SVC_subreddit_pipe = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('post_depth', Pipeline([
              ('selector', ItemSelector(key='post_depth')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,lowercase=False,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

f2_subreddit_train_features_tf_idf = f2_subreddit_pipeline.fit_transform(subreddit_train_data)
f2_subreddit_test_features_tf_df = f2_subreddit_pipeline.transform(subreddit_test_data)

# SVC Features
f2_SVC_subreddit_train_features_tf_idf = f2_SVC_subreddit_pipe.fit_transform(subreddit_train_data)
f2_SVC_subreddit_test_features_tf_df = f2_SVC_subreddit_pipe.transform(subreddit_test_data)

"""F2 Labels"""

f2_subreddit_train_labels = subreddit_train_data['subreddit']
f2_subreddit_test_labels = subreddit_test_data['subreddit']

"""**Feature 2 using SVC Classifier**"""

from sklearn.svm import SVC

f2_subreddit_svc_classifier = SVC(kernel='sigmoid',C=10)
f2_subreddit_svc_model_tf_idf = f2_subreddit_svc_classifier.fit(f2_SVC_subreddit_train_features_tf_idf, f2_subreddit_train_labels)

evaluation_summary("Feature 2: Subreddit SVC Classifier with TF-IDF",  f2_subreddit_svc_model_tf_idf.predict(f2_SVC_subreddit_test_features_tf_df),f2_subreddit_test_labels)

"""
**Feature 2 using Bernoulli Naive Bayes**
"""

from sklearn.naive_bayes import BernoulliNB

f2_subreddit_nb_classifier = BernoulliNB(alpha=0.02,fit_prior=False)

subreddit_nb_model_tf_idf = f2_subreddit_nb_classifier.fit(f2_subreddit_train_features_tf_idf,f2_subreddit_train_labels)

evaluation_summary("Feature 2: Bernoulli Naive Bayes with TF-IDF",  subreddit_nb_model_tf_idf.predict(f2_subreddit_test_features_tf_df), f2_subreddit_test_labels)

f2_subreddit_train_features_tf_idf = f2_subreddit_pipeline.fit_transform(subreddit_train_data)
f2_subreddit_test_features_tf_df = f2_subreddit_pipeline.transform(subreddit_test_data)

"""**Feature 2 using Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

f2_subreddit_dtree_classifier = DecisionTreeClassifier(random_state=1,max_features=20000)

f2_subreddit_dtree_mode_tf_idf = f2_subreddit_dtree_classifier.fit(f2_subreddit_train_features_tf_idf,f2_subreddit_train_labels)

evaluation_summary("Feature 2: Subreddit Decision Tree with TD-IDF",  f2_subreddit_dtree_mode_tf_idf.predict(f2_subreddit_test_features_tf_df), f2_subreddit_test_labels)

"""#### Combination of the two features

Combine Word Embeddings with FeatureUnion (post_depth)

"""

subreddit_train_data = f1_subreddit_train_data
subreddit_test_data = f1_subreddit_test_data
subreddit_test_data

"""Vectorizer"""

f12_subreddit_pipeline = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
            ('post_depth', Pipeline([
              ('selector', ItemSelector(key='post_depth')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,lowercase = False,sublinear_tf = True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

f12_SVC_subreddit_pipe = Pipeline([
        ('union', FeatureUnion(
          transformer_list=[
            ('title', Pipeline([
              ('selector', ItemSelector(key='title')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('body', Pipeline([
              ('selector', ItemSelector(key='body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('author', Pipeline([
              ('selector', ItemSelector(key='author')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
            ('post_depth', Pipeline([
              ('selector', ItemSelector(key='post_depth')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,lowercase=False,sublinear_tf = True)), 
              ])),
            ('Similar_Body', Pipeline([
              ('selector', ItemSelector(key='Similar_Body')),
              ('tf_idf', TfidfVectorizer(tokenizer=tokenize_normalize,max_features=5000,sublinear_tf = True)), 
              ])),
        ])
        )
    ])

f12_subreddit_train_features_tf_idf = f12_subreddit_pipeline.fit_transform(subreddit_train_data)
f12_subreddit_test_features_tf_df = f12_subreddit_pipeline.transform(subreddit_test_data)

# SVC Features
f12_SVC_subreddit_train_features_tf_idf = f12_SVC_subreddit_pipe.fit_transform(subreddit_train_data)
f12_SVC_subreddit_test_features_tf_df = f12_SVC_subreddit_pipe.transform(subreddit_test_data)

"""Labels"""

f12_subreddit_train_labels = subreddit_train_data['subreddit']
f12_subreddit_test_labels = subreddit_test_data['subreddit']

"""**Combined Features using SVC Classifier**"""

from sklearn.svm import SVC

f1_subreddit_svc_classifier = SVC(kernel='sigmoid',C=10)
f1_subreddit_svc_model_tf_idf = f1_subreddit_svc_classifier.fit(f1_SVC_subreddit_train_features, f1_subreddit_train_labels)

evaluation_summary("Feature 1: Train Data Subreddit SVC Classifier with TF-IDF",  f1_subreddit_svc_model_tf_idf.predict(f1_SVC_subreddit_test_features),f1_subreddit_test_labels, print_report = False)
evaluation_summary("Feature 1: Test Data Subreddit SVC Classifier with TF-IDF",  f1_subreddit_svc_model_tf_idf.predict(f1_SVC_subreddit_test_features),f1_subreddit_test_labels)

"""**Combined Features using BernoulliNB**"""

from sklearn.naive_bayes import BernoulliNB

f1_subreddit_nb_classifier = BernoulliNB(alpha=0.02,fit_prior=False)

subreddit_nb_model_tf_idf = f1_subreddit_nb_classifier.fit(f1_subreddit_train_features,f1_subreddit_train_labels)

evaluation_summary("Feature 1: Train Data Bernoulli Naive Bayes with TF-IDF",  subreddit_nb_model_tf_idf.predict(f1_subreddit_train_features), f1_subreddit_train_labels, print_report=False)
evaluation_summary("Feature 1: Test Data Bernoulli Naive Bayes with TF-IDF",  subreddit_nb_model_tf_idf.predict(f1_subreddit_test_features), f1_subreddit_test_labels)

"""**Combined Features using Decision Tree**"""

f1_subreddit_dtree_classifier = DecisionTreeClassifier(random_state=1,max_features=20000)

f1_subreddit_dtree_mode_tf_idf = f1_subreddit_dtree_classifier.fit(f1_subreddit_train_features,f1_subreddit_train_labels)

evaluation_summary("Feature 1: Train Data Subreddit Decision Tree with TD-IDF",  f1_subreddit_dtree_mode_tf_idf.predict(f1_subreddit_train_features), f1_subreddit_train_labels, print_report=False)
evaluation_summary("Feature 1: Test Data Subreddit Decision Tree with TD-IDF",  f1_subreddit_dtree_mode_tf_idf.predict(f1_subreddit_test_features), f1_subreddit_test_labels)